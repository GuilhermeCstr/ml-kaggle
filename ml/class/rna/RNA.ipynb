{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsXnFQbEryu7",
        "colab_type": "text"
      },
      "source": [
        "# Redes Neurais Artificiais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csuyb-xDr6wD",
        "colab_type": "text"
      },
      "source": [
        "Muito se pensa sobre como os seres humanos conseguiram evoluir, pensar, aprender. Essa investigação levou ao descobrimento do sistema nervoso humano, que é composto por uma rede conectada de neurônios.\n",
        "\n",
        "![alt text](https://static.mundoeducacao.bol.uol.com.br/mundoeducacao/2019/10/1-partes-do-neuronio.jpg)\n",
        "\n",
        "O neurônio é um tipo específico de célula que tem como função principal transmitir impulsos nervosos. Podemos resumir sua estruturas principais em dendritos, corpo celular e axônio. Os dendritos são responsáveis por conectar o nosso neurônio a outros neurônios. Quando um impulso chega a um dendrito ele transmite esse impulso para o corpo celular que em seguida passa para o axônio e este então passa o impulso para frente, criando assim uma rede conectada.\n",
        "\n",
        "Mas se o neurônio só passa informações para frente, como eles seriam responsáveis pela nossa inteligência? \n",
        "\n",
        "Uma questão que ainda não tem respostas muito definidas pela ciência é a capacidade do neurônio de aprender e de alguma forma selecionar o impulso que ele acha importante passar para frente, intensificar ou suprimir. É dessa forma que são criados caminhos de comunicação no nosso sistema nervoso, que quando bem treinado consegue reconhecer muito bem os padrões estabelecidos.\n",
        "\n",
        "Depois de entender como funcionam as redes neurais biológicas, os cientistas começaram a tentar reproduzir esse comportamento em um modelo matemático que conseguisse aprender assim como um ser humano. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25oHOZUNU4hd",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxfO71QZ_wV9",
        "colab_type": "text"
      },
      "source": [
        "O [Perceptron](http://deeplearningbook.com.br/o-perceptron-parte-1/), desenvolvido pelo cientista Frank Rosenblatt, foi um dos primeiros modelos propostos para resolver essa questão.\n",
        "\n",
        "\n",
        "<img src=\"perceptron.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "Nesse modelo, a saída é o resultado de:\n",
        "\n",
        "$$ Saida=\\begin{cases} 0,\\quad se\\quad \\sum _{ i=1 }^{ n }{ { x }_{ i }{ w }_{ i } \\le  limiar }  \\\\ 1,\\quad se\\quad \\sum _{ i=1 }^{ n }{ { x }_{ i }{ w }_{ i } > limiar }  \\end{cases} $$\n",
        "\n",
        "\n",
        "Fazendo um paralelo com o neurônio humano, os impulsos que chegam dos neunônios vizinhos são os Xi, os dendritos podem ser comparados com os pesos Wi, o corpo celular faz o somatório e aplica o limiar e o axônio transmite a saída que será entrada do próximo neurônio na rede.\n",
        "\n",
        "A equação do perceptron pode ser reescrita como:\n",
        "\n",
        "$$ Saida=\\begin{cases} 0,\\quad se\\quad x\\cdot w-b\\le 0 \\\\ 1,\\quad se\\quad x\\cdot w-b>0\\quad  \\end{cases} $$\n",
        "\n",
        "Em que o produdo de x por w é o produto escalar dos vetores de entrada e de pesos e b é o bias do perceptron, que representa o limiar da equação anterior.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WMxvX9-iIWu",
        "colab_type": "text"
      },
      "source": [
        "O [modelo de neurônio](http://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/) mais utilizado nas redes neurais modernas é um pouco diferente do perceptron. E permite a utilização de uma função de ativação, que tem grande importância na etapa de treinamento da rede.\n",
        "\n",
        "Sendo assim, o modelo matemático do neurônio fica:\n",
        "\n",
        "$$ Saida=f\\left( \\left( \\sum _{ i=1 }^{ n }{ { x }_{ i }{ w }_{ i } }  \\right) +b \\right) $$\n",
        "\n",
        "Em que f é a função de ativação.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUxGhSiHUtqP",
        "colab_type": "text"
      },
      "source": [
        "## Redes Neurais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NvugzsWcYuN",
        "colab_type": "text"
      },
      "source": [
        "Para que o algoritmo possa aprender padrões complexos de um conjunto de dados, muitas vezes um perceptron não é suficiente. Assim, são criadas redes neurais que são conjuntos de neurônios interligados.\n",
        "\n",
        "\n",
        "<img src=\"https://lamfo-unb.github.io/img/simple_DNN/RNA1.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "A imagem acima representa um rede neural de três camadas. A camada mais à esquerda (verde) é chamada de camada de entrada. A mais à direita (vermelha) é chamada de camada de saída, que, nesse caso, contém apenas um neurônio. A camada do meio (azul) é chamada de camada oculta, que significa simplesmente que essa camada não é entrada nem saída. A rede acima possui penas uma camada oculta, mas existem redes que possuem múltiplas camadas ocultas.\n",
        "\n",
        "O design das camadas de entrada e saída é bem direto. Por exemplo, suponha que queremos determinar se, dadas condições, vai chover ou não. Os atributos são: umidade do ar, temperatura, época do ano e quantidade de nuvens no céu. Uma forma natural de projetar essa rede é codificar os valores numéricos dos atributos nos neurônios de entrada. Como são quatro atributos, serão quatro neurônios de entrada. A camada de saída irá conter apenas um neurônio com valores inferiores a 0,5 indicando que não vai chover e valores superiores a 0,5 indicando que vai chover.\n",
        "\n",
        "O design das camadas ocultas é mais variado e pesquisadores de redes neurais desenvolveram várias heurísticas para o design dessas camadas para que a rede possua o comportamento desejado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFr7UYX6l_P",
        "colab_type": "text"
      },
      "source": [
        "### Redes Neurais Feed-Forward\n",
        "As redes neurais feed-foward são as mais comuns na prática. Nelas, a primeira camada é a de entrada e a última a de saída. Se houver mais de uma camada oculta, são chamadas de redes neurais profundas. Nesse modelo, a saída de uma camada é sempre entrada da camada seguinte, ou seja, não existem loops na rede. As atividades dos neurônios em cada camada são uma função não-linear das atividades na camada anterior.\n",
        "\n",
        "<img src=\"https://static.imasters.com.br/wp-content/uploads/2017/09/1-1.png\" alt=\"drawing\" width=\"300\"/>\n",
        "\n",
        "\n",
        "As redes neurais multicamadas tratam-se dos casos em que a camada oculta da rede, já descrita anteriormente, é formada por inúmeras camadas. Em outras palavras, são arquiteturas onde os neurônios são organizados em duas ou mais camadas de processamento (camadas ocultas), já que sempre vai existir pelo menos uma camada de entrada e uma camada de saída. Construindo-se, assim, uma verdadeira grande rede de camadas de neurônios para que seja  possível tratar problemas baseados em dados que não são linearmente separáveis.\n",
        "\n",
        "\n",
        "#### Separação de Dados: Linear x Não-Linear\n",
        "\n",
        "Dados considerados linearmente separáveis são ditos dados que, se representados em um gráfico de duas dimensões, será possível traçar uma reta que separe as classes. Para ilustrar, temos a imagem abaixo, em que é possível traçar, fácilmente, uma reta que separa as duas classes apresentadas.\n",
        "\n",
        "Para esse tipo de dataset, redes neurais com apenas uma única camada oculta conseguem desempenhar bons resultados, tendo em vista a certa simplicidade do problema.\n",
        "\n",
        "![Linearmente separável](https://jamesmccaffrey.files.wordpress.com/2019/04/linearly_sep_data.jpg?w=900)\n",
        "\n",
        "\n",
        "Por outro lado, dados considerados não-linearmente separáveis apresentam uma disposição de classes, quando representados em um gráfico, em que não se torna possível a distinção das mesmas apenas com uma reta. Como ilustrado abaixo, para separação das classes, temos que o gráfico à esquerda necessitaria de duas retas, enquanto que o à direita necessitaria de uma curva.\n",
        "\n",
        "Já nesse caso, a aplicação de uma rede neural com apenas uma única camada oculta não é satisfatória. Pelo aumento de complexidade, a acurácia de um rede de única camada decai significativamente.\n",
        "\n",
        "![não-linearmente separável](https://jamesmccaffrey.files.wordpress.com/2019/04/linearly_sep_data_not.jpg?w=900)\n",
        "\n",
        "#### Características\n",
        "\n",
        "Como já citado, tal modelo é extremamente util para resolução de problemas não-lineares, tornando possível o desenvolvimento de estudos com desafios mais complexos.\n",
        "\n",
        "Mas, isso também pode ser tratado como uma faca de dois gumes. Com o aumento da complexidade da rede, adicionando inúmeras camadas de processamento, aumenta-se, também, o nível de dificuldade para otimizá-la quando trabalho com datasets extremamente grandes. \n",
        "\n",
        "O simples ato de aumentar o número de camadas e neurônios pode não acarretar em uma melhora na acurácia das soluções. Uma das limitações dessa rede é que ao se aumentar muito o número de camadas e neurônios, ela tende a ficar com um número de parâmetros exagerado e, com isso, tão complexa e pesada a ponto do hardware ter dificuldades em processa-la e ela não convergir.\n",
        "\n",
        "Outro fator é que esse tipo de rede possui uma função de perda não convexa, onde existe mais de um mínimo local. Portanto, diferentes inicializações de pesos aleatórios podem levar a uma precisão de validação diferente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nmaHVj4lWvO",
        "colab_type": "text"
      },
      "source": [
        "### Redes Neurais Recorrentes\n",
        "\n",
        "Nesse modelo, a retroalimentação é possível. Essas redes podem ter uma dinâmica complicada, o que dificulta o treinamento, contudo, elas são mais biologicamente realistas. \n",
        "\n",
        "Nas redes recorrentes, a ideia é ter neurônios que disparem por um tempo limitado, estimulando outros neurônios que também disparem por tempo limitado. Dessa forma, ocorrerá uma cascata de disparos, fazendo com que loops não causem prejuízos à rede, uma vez que a saída de um neurônio afeta apenas sua entrada em algum momento posterior, não instantaneamente.\n",
        "\n",
        "<img src=\"https://static.imasters.com.br/wp-content/uploads/2017/09/2-1.png\" alt=\"drawing\" width=\"300\"/>\n",
        "\n",
        "As redes neurais simétricas tratam-se de um caso particular das redes recorrentes em que as conexões entre as unidades são simétricas, ou seja, a matriz de conectividade é simétrica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVkPUoTYD7iN",
        "colab_type": "text"
      },
      "source": [
        "## Redes Neurais Convolucionais\n",
        "\n",
        "Uma rede neural convolucional é um algoritmo de aprendizado de máquina muito usado para se trabalhar com imagens. Ela é capaz de, atribuindo importância a vários aspectos, diferenciar uma imagem de outra.\n",
        "\n",
        "Redes neurais convolucionais são bastante parecidas com redes neurais simples, pois, da mesma forma, possuem pesos e bias. Porém, diferentemente das redes apresentadas anteriormente, não há um peso para cada neurônio que o conecte com a próxima camada, ou seja, uma camada completamente conectada, e sim, um conjunto de pesos com que é feita uma operação de convolução. Dessa forma, a quantidade de pesos a serem calculados é bem menor. \n",
        "\n",
        "Além disso, redes convolucionais são capazes de aprender o comportamento de filtros usados para obter features, diminuindo assim o pré processamento.\n",
        "\n",
        "A seguir, serão explicadas algumas camadas de uma rede neural convolucional.\n",
        "\n",
        "### Camada Convolucional\n",
        "\n",
        "Como o nome já diz, é nessa camada em que ocorre a convolução. É definido um tamanho de kernel e a quantidade. A convolução será aplicada usando e, para cada kernel, um mapa de características será gerado.\n",
        "\n",
        "### Camada de Pooling\n",
        "\n",
        "Essa camada é aplicada posteriormente a uma camada convolucional. Seu intuito pricipal é condensar as infromações obtidas na camada anterior. Ela recebe como entrada o mapa de características da camada convolucional e prepara um mapa de características condensadas. Dessa forma, cada unidade da camada de pooling resume uma região da camada que deu origem a ela. \n",
        "\n",
        "Uma abordagem comum para o pooling é o Max-pooling, em que, para cada região, geralmente 2x2, a maior ativação é considerado. Dessa forma, o tamanho do mapa de recurso é reduzido a um quarto de seu tamanhgo original. Contudo, esse não é o único procedimento, outro comum é o chamado Pooling L2, em que o valor final corresponde a à raiz quadrada da soma dos quadrados das ativações.\n",
        "\n",
        "<img src=\"https://perso.mines-paristech.fr/fabien.moutarde/ES_MachineLearning/TP_convNets/pool.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "### Camada de Saída\n",
        "\n",
        "Essa é a camda final da rede. Se for um problema de classificação ou regressão, é ideal que essa camada não seja unidimensional, assim, muitas vezes, é preciso ter uma tranformação antes de se chegar a essa camadaa. Além disso, geralmente, essa camada é totalmente conectada com a anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQycA-tTI9K9",
        "colab_type": "text"
      },
      "source": [
        "# Aplicações com Redes Neurais Artificiais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQlqcCuaePZY",
        "colab_type": "text"
      },
      "source": [
        "## Rede Neural Multicamadas com Scikit-Learn\n",
        "\n",
        "Para a implementação de uma rede neural desse tipo, é aconselhável a utilização de bibliotecas que foram desenvolvidas para aplicação de tais métodos. \n",
        "\n",
        "Escolhemos a base [MNIST](http://yann.lecun.com/exdb/mnist/) para exemplificar o uso da biblioteca Scikit Learn no processamento e obtenção de resultados usando o [MPLClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), que é a implementação do Scikit para uma rede neural multicamada. Os arquivos da base devem ser [baixados](https://www.kaggle.com/oddrationale/mnist-in-csv) e colocados no mesmo diretório que esse código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuOLhv4ieO0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMsFLkN-TZsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criamos o dataframe\n",
        "# Para isso, é importado os arquivos .csv de treinamento e teste\n",
        "\n",
        "path = \"mnist_train.csv\"\n",
        "mnist_data = pd.read_csv(path)\n",
        "\n",
        "path_test = \"mnist_test.csv\"\n",
        "mnist_test = pd.read_csv(path_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZTrT_ocqAyh",
        "colab_type": "text"
      },
      "source": [
        "Agora, podemos verificar como os dados estão dispostos no dataframe.\n",
        "\n",
        "Temos que a primeira coluna representa a classe da imagem, enquanto que as colunas seguintes representam o valor de cada pixel da imagem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSOObFPEb8Vd",
        "colab_type": "code",
        "outputId": "66f2a40d-8a8e-425b-c745-6cd3f7d0c06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(mnist_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       5  0  0.1  0.2  0.3  0.4  ...  0.612  0.613  0.614  0.615  0.616  0.617\n",
            "0      0  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "1      4  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "2      1  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "3      9  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "4      2  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "...   .. ..  ...  ...  ...  ...  ...    ...    ...    ...    ...    ...    ...\n",
            "59994  8  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "59995  3  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "59996  5  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "59997  6  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "59998  8  0    0    0    0    0  ...      0      0      0      0      0      0\n",
            "\n",
            "[59999 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTa9PGkUkyVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Escolhemos os dados necessários para o treinamento para a variavel X\n",
        "X_train = mnist_data.iloc[:, 1:]\n",
        "X_test = mnist_test.iloc[:, 1:]\n",
        "\n",
        "# Colocamos a coluna de classes na variavel y\n",
        "y_train = mnist_data.iloc[:, 0]\n",
        "y_test = mnist_test.iloc[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_k6BdScbR_Q",
        "colab_type": "text"
      },
      "source": [
        "Agora que estamos já preparados, podemos realizar o treinamento em si, utilizando a função que executa a rede neural multicamadas (MLPClassifier).\n",
        "\n",
        "Para entender melhor seu funcionamento, devemos conhecer seus parâmetros:\n",
        "\n",
        "\n",
        "*   hidden_layer_sizes: Permite escolher o número de camadas e neurônios das camadas ocultas.\n",
        "*   max_iter: Define número de épocas. Default = 200.\n",
        "*   solver: Método de solução dos pesos, sendo ‘lbfgs’, ‘sgd’ ou ‘adam’. No geral, conseguem os mesmos resultados, mas cada um é mais indicado dependendo do tamanho do dataset. Default = ’adam’.\n",
        "* learning_rate: A taxa de aprendizagem pode ser definida como: ‘constant’, taxa constante, ‘invscaling’, taxa diminui a cada época por um fator determinado, e ‘adaptive’, a taxa diminui quando a função de erro para de diminuir. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx45tZRslYAS",
        "colab_type": "code",
        "outputId": "8161a70d-23e8-4380-f2ee-87f00f51b8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Escolhemos duas camadas ocultas com tamanhos 100 e 20 neurônios, e o numero máximo de iterações como 3000.\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(1000, 20), max_iter=3000)\n",
        "# Executamos o treinamento\n",
        "mlp.fit(X_train, y_train.values.ravel())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(50, 10, 10), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LN5BbrPvgs3",
        "colab": {}
      },
      "source": [
        "# Executamos as predições\n",
        "mlp_prediction = mlp.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkKc_0VNfeyp",
        "colab_type": "text"
      },
      "source": [
        "Podemos realizar a verificação dos resultados através das funções de métricas presentes na biblioteca Sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc6PhzJElf6k",
        "colab_type": "code",
        "outputId": "4e70fa67-1ee2-4d20-d666-d48a640f9f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# Podemos imprimir a matriz de confusão encontrada\n",
        "print(confusion_matrix(y_test,mlp_prediction))\n",
        "\n",
        "# E também um relatório completo com cada métrica para análise\n",
        "print(classification_report(y_test,mlp_prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 971    0    3    0    0    0    3    1    2    0]\n",
            " [   1 1099   11    4    0    0    2    2   16    0]\n",
            " [   1    0 1020    4    2    0    0    2    3    0]\n",
            " [   0    0    7  993    1    5    0    1    3    0]\n",
            " [   4    0    2    1  961    1    4    5    1    3]\n",
            " [   6    0    2   17    1  858    2    0    4    2]\n",
            " [   9    2    0    1    4    9  931    0    2    0]\n",
            " [   0    1   13   11    7    0    0  985    0   10]\n",
            " [   7    1   10    5    2    5    3    1  932    8]\n",
            " [   5    1    1    9   18    1    0    4    6  964]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       980\n",
            "           1       1.00      0.97      0.98      1135\n",
            "           2       0.95      0.99      0.97      1032\n",
            "           3       0.95      0.98      0.97      1010\n",
            "           4       0.96      0.98      0.97       982\n",
            "           5       0.98      0.96      0.97       892\n",
            "           6       0.99      0.97      0.98       958\n",
            "           7       0.98      0.96      0.97      1027\n",
            "           8       0.96      0.96      0.96       974\n",
            "           9       0.98      0.96      0.97      1009\n",
            "\n",
            "    accuracy                           0.97      9999\n",
            "   macro avg       0.97      0.97      0.97      9999\n",
            "weighted avg       0.97      0.97      0.97      9999\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35151578-e236-49ae-f006-ae72b84e7917",
        "id": "4khYgKYJ2CJU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Por fim, imprimimos apenas a acurácia alcançada, para visualização de um maior número de casas decimais\n",
        "print(\"A acurácia foi de\",accuracy_score(y_test, mlp_prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A acurácia foi de 0.9714971497149715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qeucEbxF1-Xz"
      },
      "source": [
        "**Nos** testes realizados, foram encontrados:\n",
        "\n",
        "\n",
        "*   Rede com 300 camadas ocultas com 20 neurônios cada - Acurácia: 97,40%\n",
        "*   Rede com 100 camadas ocultas com 20 neurônios cada - Acurácia: 97,21%\n",
        "*   Rede com 100 camadas ocultas com 10 neurônios cada - Acurácia: 96,14%\n",
        "*   Rede com 1000 camadas ocultas com 20 neurônios cada - Acurácia: 97,14%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nivI8O_XY00d"
      },
      "source": [
        "## Exemplo de Rede Convolucional usando Keras\n",
        "\n",
        "A seguir temos um exemplo de rede convolucional usando a biblioteca [Keras](https://keras.io/). Essa biblioteca usa [Tensorflow](https://www.tensorflow.org/overview) como backend e foi escolhida por ter implemetadas formas fáceis e poderosas de se criar uma rede convolucional.\n",
        "\n",
        "\n",
        "Como redes convolucionais são muito boas para se trabalhar com imagens, foi escolhida uma base de dados de imagens para ser usada no exemplo.\n",
        "A base de dados usada foi a [COVID-19 Radiography Database](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database), que contém imagens de raio-X de pacientes com Covid-19, pacientes saudáveis e pacientes com pneumonia causada por outro tipos de vírus. \n",
        "\n",
        "As imagens foram separadas em treinamento, aproximadamente 50% do total, validação, 25%, e teste, 25%, sempre mantendo a proporção das classes. As imagens de treinamento foram colocadas em um diretório chamado \"train\" e separadas por classe em sub-diretórios diferentes. As imagens de validação foram colocadas em um diretório chamado \"validation\". E as de teste foram colocadas no diretório \"test\", também separadas por classe em sub-diretórios. Todas as pastas devem estar no mesmo diretório que o código.\n",
        "\n",
        "O código foi testado em uma máquina com processador Intel core i7 64bits, 32GiB de memoria, sistema operacional Linux Xubuntu 20.04 64 bits e keras com tensorflow em backend versão 2.3.0 e tensorflow versão 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMGxqew1ZVZI",
        "colab_type": "code",
        "outputId": "be669573-d868-4fa4-c6ec-9f68d37bec00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hqRhmNxvdX",
        "colab_type": "code",
        "outputId": "b786d38f-c5d5-4078-dccb-0fde690fd0c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "# Inicializando um modelo sequencial\n",
        "model = Sequential()\n",
        "\n",
        "# Camada convolucional\n",
        "model.add(Conv2D(filters=32, kernel_size=(7,7), input_shape=(128,128,1), activation='relu'))\n",
        "\n",
        "# Camada de pooling\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adicionando mais camadas\n",
        "model.add(Conv2D(filters=20, kernel_size=(3, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Transformando as camadas 2D para 1D\n",
        "model.add(Flatten())\n",
        "\n",
        "# Deixando a camada totalmente conectada\n",
        "model.add(Dense(units = 100, activation = 'relu'))\n",
        "model.add(Dense(units = 3, activation = 'sigmoid'))\n",
        "\n",
        "# Compilando a rede\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 122, 122, 32)      1600      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 61, 61, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 59, 59, 20)        5780      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 20)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16820)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               1682100   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 1,689,783\n",
            "Trainable params: 1,689,783\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd9rKSb7Y6jl",
        "colab": {}
      },
      "source": [
        "# Pre processamento das imagens\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2,zoom_range = 0.2, horizontal_flip = True)\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('train',target_size = (128, 128), batch_size = 32, class_mode = 'categorical', color_mode='grayscale')\n",
        "validation_set = validation_datagen.flow_from_directory('validation',target_size = (128, 128), batch_size = 32, class_mode = 'categorical', color_mode='grayscale')\n",
        "\n",
        "print('*********************INICIO DO TREINAMENTO*************************')\n",
        "\n",
        "# Executando o treinamento\n",
        "model.fit(training_set, epochs = 50, validation_data = validation_set, validation_steps = 20)\n",
        "model.save_weights('cnn_covid.h5')\n",
        "\n",
        "print('*********************FIM DO TREINAMENTO*************************')\n",
        "\n",
        "\n",
        "# Etapa de teste\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('test',target_size = (128, 128), batch_size = 32, class_mode = 'categorical', color_mode='grayscale')\n",
        "\n",
        "print('*********************INICIO DO TESTE*************************')\n",
        "\n",
        "score = model.evaluate(test_set, verbose=True)\n",
        "\n",
        "print('*********************FIM DO TESTE*************************')\n",
        "\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t5kg9I5-Y-4R"
      },
      "source": [
        "Ao fim do processamento na máquina descrita acima, obtivemos:\n",
        "* Acurácia da validação: 0.9443\n",
        "* Acurácia do teste: 0.9133\n",
        "\n",
        "O tempo de treinamento foi de aproximadamente 17min com uma média de 20s por época."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1usu4h0fCKh1",
        "colab_type": "text"
      },
      "source": [
        "## Mãos a Obra: Como implementar sua própria rede neural?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7j6TYdCiBQ",
        "colab_type": "text"
      },
      "source": [
        "Para quem gosta de entender como as coisas funcionam de verdade, faremos aqui uma implementação de uma rede neural simples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYeK5fqeDPm3",
        "colab_type": "text"
      },
      "source": [
        "### Camada\n",
        "\n",
        "A estrutura básica da nossa implementação da rede neural é a camada. A camada tem como componentes básicos a matriz de pesos e o vetor de bias.\n",
        "\n",
        "\n",
        "![alt text](camada.png)\n",
        "\n",
        "Na matriz de pesos, cada linha representa um neurônio dessa camada. E cada elemento dessa linha representa um peso para dada entrada da camada, que são inicializados de forma aleatória. O vetor de bias é uma matriz coluna em que cada elemento é o bias de um velor da camada.\n",
        "\n",
        "```\n",
        "class Layer:\n",
        "  def __init__(self, n_neurons, n_inputs, is_output_layer=False):\n",
        "    # Número de neurônios da camada\n",
        "    self.size = n_neurons\n",
        "    # Matriz de pesos\n",
        "    self.weights = np.random.rand(n_neurons, n_inputs,)\n",
        "    # Vetor de bias\n",
        "    self.biases = np.random.rand(n_neurons,1)\n",
        "    self.is_output_layer = is_output_layer\n",
        "```\n",
        "\n",
        "\n",
        "#### **Forward Pass**\n",
        "\n",
        "Forward pass é nome que se dá para a função que faz a operação:\n",
        "\n",
        "$$ Saida=f\\left( \\left( \\sum _{ i=1 }^{ n }{ { x }_{ i }{ w }_{ i } }  \\right) +b \\right) $$\n",
        "\n",
        "em todos os neurônios da rede. Essa operação nada mais é que a multiplicação da matriz de pesos da camada pelo vetor de entrada dessa camada e a aplicação da função de ativação, que no nosso caso é a função sigmoide.\n",
        "\n",
        "Na nossa implementação, para poupar trabalho na etapa de backward pass, também calculamos a derivada da função de ativação aplicada na saída da camada.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  def forward_pass(self, inputs):\n",
        "    def sigmoid(x): \n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    # Aplica função em todos os elementos do vetor\n",
        "    sigmoid_v = np.vectorize(sigmoid)\n",
        "\n",
        "    def sigmoid_derivative(x): \n",
        "      return x * (1 - x)\n",
        "    sigmoid_derivative_v = np.vectorize(sigmoid_derivative)\n",
        "\n",
        "    # Aplicação dos pesos e bias de cada neurônio\n",
        "    z = np.dot(self.weights, inputs) + self.biases\n",
        "    # Cálculo da saída da rede\n",
        "    self.output = sigmoid_v(z)\n",
        "    # Cálculo da derivada da função de ativação aplicada na saída\n",
        "    self.delta_output = sigmoid_derivative_v(self.output)\n",
        "\n",
        "```\n",
        "\n",
        "#### **Backward Pass**\n",
        "\n",
        "A função backward_pass realiza o algoritmo de *Backpropagation* em cada camada da nossa rede.\n",
        "\n",
        "A princípio, os pesos e bias utilizados para as primeiras predições foram determinados de forma aleatória, causando divergências nos valores esperados como saída da rede. Tendo em vista o erro encontrado no resultado final, podemos utilizar o método de retro-propagação para que os pesos em cada neurônio sejam ajustados de forma a alcançar os valores ideiais que resultariam em predições corretas. Sendo bastante improvável que seja alcançado valores aceitaveis em uma primeira retro-propagação, todo esse ciclo de propagação e retro-propagação é repetido n-vezes até que o erro produzido seja pequeno o suficiente.\n",
        "\n",
        "A [formulação matemática](https://towardsdatascience.com/the-maths-behind-back-propagation-cf6714736abf) desse algoritmo é um pouco complicada, aqui iremos mostrar apenas a sua implementação.\n",
        "\n",
        "A função de backward_pass irá passar pela rede de trás para frente calculando o erro que a sua operação gerou no resultado final. Esse erro irá mais tarde guiar quanto e em que sentido a rede deve alterar seus pesos para diminuir o valor da função de custo. \n",
        "\n",
        "A função de custo que utilizamos é:\n",
        "\n",
        "$$ Funcao\\_ de\\_ Custo = \\frac { 1 }{ 2 } \\sum _{ i=1 }^{ n }{ { (saida-valor\\_ esperado) }^{ 2 } }  $$\n",
        "\n",
        "Usamos a derivada da função de custo para calcular o delta que indica o quanto a variação de cada peso irá afetar a saída.\n",
        "\n",
        "Esse delta é:\n",
        "\n",
        "$$ delta = derivada\\_ funcao\\_ de\\_ custo\\cdot derivada\\_ funcao\\_ de\\_ ativacao $$\n",
        "\n",
        "Para a camada de saída essa derivada é:\n",
        "\n",
        "$$ derivada\\_ funcao\\_ de\\_ custo = saida-valor\\_ esperado $$\n",
        "\n",
        "Para as demais camadas:\n",
        "\n",
        "$$ derivada\\_ funcao\\_ de\\_ custo = matriz\\_ de\\_ pesos\\cdot delta\\_ camada\\_ a\\_ direita $$\n",
        "\n",
        "A derivada da função de ativação foi calculada e armazenada na função forward pass.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  def backward_pass(self, y, rightLayer):\n",
        "    # Para a camada de saída\n",
        "    if self.is_output_layer:\n",
        "      # Derivada da função de custo\n",
        "      error =  self.output - y\n",
        "      self.delta = np.atleast_2d(error * self.delta_output)\n",
        "    # Para as outras camadas\n",
        "    else:\n",
        "      self.delta = np.atleast_2d(rightLayer.weights.T.dot(rightLayer.delta)*self.delta_output)\n",
        "    return self.delta\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **Update**\n",
        "\n",
        "A função update realiza a atualização dos pesos e bias da camada, com base em:\n",
        "\n",
        "$$ novos\\_ pesos=taxa\\_ de\\_ aprendizagem\\cdot (delta\\cdot entrada\\_ da\\_ camada) $$\n",
        "\n",
        "$$ novos\\_ bias=taxa\\_ de\\_ aprendizagem\\cdot delta $$\n",
        "\n",
        "```\n",
        " def update(self, learning_rate, left_output):\n",
        "    layer_input = np.atleast_2d(left_output)\n",
        "    delta = np.atleast_2d(self.delta)\n",
        "    self.weights -= learning_rate * delta.dot(layer_input.T)\n",
        "    self.biases -= learning_rate * delta\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64wU0ABgVNbr",
        "colab_type": "text"
      },
      "source": [
        "Assim, temos a classe Layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM1A8oYaUfVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  def __init__(self, n_neurons, n_inputs, is_output_layer=False):\n",
        "    # Número de neurônios da camada\n",
        "    self.size = n_neurons\n",
        "    # Matriz de pesos\n",
        "    self.weights = np.random.rand(n_neurons, n_inputs,)\n",
        "    # Vetor de bias\n",
        "    self.biases = np.random.rand(n_neurons,1)\n",
        "    self.is_output_layer = is_output_layer\n",
        "    \n",
        "  def forward_pass(self, inputs):\n",
        "    def sigmoid(x): \n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    # Aplica função em todos os elementos do vetor\n",
        "    sigmoid_v = np.vectorize(sigmoid)\n",
        "\n",
        "    def sigmoid_derivative(x): \n",
        "      return x * (1 - x)\n",
        "    sigmoid_derivative_v = np.vectorize(sigmoid_derivative)\n",
        "\n",
        "    # Aplicação dos pesos e bias de cada neurônio\n",
        "    z = np.dot(self.weights, inputs) + self.biases\n",
        "    # Cálculo da saída da rede\n",
        "    self.output = sigmoid_v(z)\n",
        "    # Cálculo da derivada da função de ativação aplicada na saída\n",
        "    self.delta_output = sigmoid_derivative_v(self.output)\n",
        "\n",
        "  def backward_pass(self, y, rightLayer):\n",
        "    # Para a camada de saída\n",
        "    if self.is_output_layer:\n",
        "      # Derivada da função de custo\n",
        "      error =  self.output - y\n",
        "      self.delta = np.atleast_2d(error * self.delta_output)\n",
        "    # Para as outras camadas\n",
        "    else:\n",
        "      self.delta = np.atleast_2d(rightLayer.weights.T.dot(rightLayer.delta)*self.delta_output)\n",
        "    return self.delta\n",
        "\n",
        "  def update(self, learning_rate, left_output):\n",
        "    layer_input = np.atleast_2d(left_output)\n",
        "    delta = np.atleast_2d(self.delta)\n",
        "    self.weights -= learning_rate * delta.dot(layer_input.T)\n",
        "    self.biases -= learning_rate * delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482FiQouVUxI",
        "colab_type": "text"
      },
      "source": [
        "### Rede Neural\n",
        "\n",
        "A classe Network monta a rede neural usando as camadas. Ela recebe o tamanho do vetor de entradas, um vetor de inteiros que indica a quantidade de neurônios de cada camada e a taxa de aprendizagem da rede."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U740Xin35TSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "  def __init__(self, n_inputs, layers_size, learning_rate):\n",
        "    # A rede é composta por uma lista de camadas\n",
        "    self.layers = [] \n",
        "    # Número de camadas\n",
        "    self.size = len(layers_size)\n",
        "    # Taxa de Aprendizagem\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    # Preenche a rede com camadas\n",
        "    for i in range(len(layers_size)):\n",
        "      if i == 0: # Primeira camada\n",
        "        self.layers.append(Layer(layers_size[i], n_inputs))\n",
        "      elif i == len(layers_size)-1: # Última camada\n",
        "        self.layers.append(Layer(layers_size[i], layers_size[i-1], True))\n",
        "      else: # Demais Camadas\n",
        "        self.layers.append(Layer(layers_size[i], layers_size[i-1]))\n",
        "\n",
        "  # Aplica o algoritmo de backpropagation para um exemplo de treinamento.\n",
        "  def train(self, x, y):\n",
        "    # Aplica forward_pass\n",
        "    for i in range(self.size):\n",
        "      if i == 0:\n",
        "         self.layers[i].forward_pass(x)\n",
        "      else:\n",
        "         self.layers[i].forward_pass(self.layers[i-1].output)\n",
        "    # Aplica backward_pass\n",
        "    for i in range(1, self.size+1):\n",
        "      if i == 1:\n",
        "        self.layers[-i].backward_pass(y, None)\n",
        "      else:\n",
        "        self.layers[-i].backward_pass(y, self.layers[-i+1])\n",
        "    # Aplica update\n",
        "    for i in range(1, self.size+1):\n",
        "      if i == self.size:\n",
        "        self.layers[-i].update(self.learning_rate, x)\n",
        "      else:\n",
        "        self.layers[-i].update(self.learning_rate, self.layers[-i-1].output)\n",
        "\n",
        "    # Retorna o valor da função de custo para o exemplo de treinamento\n",
        "    return  sum(np.exp2(self.layers[-1].output - y))/2\n",
        "\n",
        "  # Aplica o treinamento para vários exemplos iterando o número de épocas dadas\n",
        "  def fit(self, x, y, epochs):\n",
        "\n",
        "    for j in range(epochs):\n",
        "      error = 0\n",
        "      for i in range(len(x)):\n",
        "        inputs = np.resize(x[i], (len(x[i]),1))\n",
        "        outputs = np.resize(y[i], (len(y[i]),1))\n",
        "        error += self.train(inputs,outputs)\n",
        "\n",
        "      # Média do resultado da função de custo para todos os exemplos de treinamento  \n",
        "      error = error/len(x)\n",
        "\n",
        "      self.predict(x,y)\n",
        "    print(\"Loss {} Acurácia {}%\".format(error, self.accuracy*100))\n",
        "\n",
        "\n",
        "  # Faz a predição dos exemplos de treinamento aplicados nos valores atuais dos pesos da rede\n",
        "  def predict(self, x, y):\n",
        "    accuracy = 0\n",
        "  \n",
        "    for j in range(len(x)):\n",
        "      inputs = np.resize(x[j], (len(x[j]),1))\n",
        "      outputs = np.resize(y[j], (len(y[j]),1))\n",
        "\n",
        "      # Aplica forward_pass\n",
        "      for i in range(self.size):\n",
        "        if i == 0:\n",
        "          self.layers[i].forward_pass(inputs)\n",
        "        else:\n",
        "          self.layers[i].forward_pass(self.layers[i-1].output)\n",
        "\n",
        "      y_pred = self.layers[-1].output.flatten()\n",
        "      y_real = y[j].flatten()\n",
        "\n",
        "      # Compara saída com resultado esperado\n",
        "      if self.classify(y_pred, y_real):\n",
        "        accuracy += 1\n",
        "\n",
        "    self.accuracy = accuracy/len(x)\n",
        "    return self.accuracy\n",
        "\n",
        "  \n",
        "  def classify(self, y_pred, y_real):\n",
        "    # Se a sua saída é binária, aplica limiar\n",
        "    if(len(y_pred)==1):\n",
        "      if(y_pred[0]>0.5 and y_real[0]==1) or (y_pred[0]<0.5 and y_real[0]==0):\n",
        "        return True\n",
        "      return False\n",
        "    # Se a saída é um vetor, verifica qual elemento possui maior probabilidade\n",
        "    return np.argmax(y_pred) == np.argmax(y_real)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ycgyq_kVFuH",
        "colab_type": "text"
      },
      "source": [
        "### Testando a Rede"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSVVLTT9Yi6O",
        "colab_type": "text"
      },
      "source": [
        "Para testar o funcionamento da nossa rede, vamos aplicá-la na base íris e verificar se após o treinamento, ela consegue reduzir o erro e aprender a diferenciar as espécies da flor íris."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2keRx0y6dGM",
        "colab_type": "code",
        "outputId": "cbfcc43e-4e48-4e52-f8a6-f1746f44ad3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets \n",
        "from pandas import get_dummies\n",
        "\n",
        "# Carregamos o dataset\n",
        "iris = datasets.load_iris()\n",
        "x = np.array(iris.data[:, :4])  # we only take the first two features.\n",
        "y = np.array(get_dummies(iris.target).values)\n",
        "\n",
        "# A rede irá receber como entrada um vetor de 4 elementos\n",
        "input_size = 4\n",
        "# A rede possui uma cama oculta de 5 neurônios e uma camada de saída de 3 neurônios,\n",
        "# que indicam as 3 espécies da base iris\n",
        "layers_size = [5, 3]\n",
        "\n",
        "\n",
        "net = Network(input_size, layers_size, 0.1)\n",
        "net.fit(x,y,300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss [1.50664466] Acurácia 96.66666666666667%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTzA-gkvaMqw",
        "colab_type": "text"
      },
      "source": [
        "Após 300 épocas, nossa rede consegue acertar 96% dos exemplos de treinamento. Esse número pode variar a cada teste porque os pesos iniciais são aleatórios. O número de épocas também pode melhorar a precisão do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZLUc4sNbrU0",
        "colab_type": "text"
      },
      "source": [
        "# Referências Bibliográficas\n",
        "\n",
        "* http://deeplearningbook.com.br/o-perceptron-parte-1/\n",
        "* https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\n",
        "* http://deeplearningbook.com.br/a-arquitetura-das-redes-neurais/\n",
        "* https://lamfo-unb.github.io/2017/06/18/itro-ao-deep-learning/\n",
        "* https://www.youtube.com/watch?v=aircAruvnKk\n",
        "* https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
        "* https://imasters.com.br/data/um-mergulho-profundo-nas-redes-neurais-recorrentes\n",
        "* http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/\n",
        "* https://jamesmccaffrey.wordpress.com/2019/04/27/the-difference-between-linearly-separable-data-and-a-linear-classifier-in-machine-learning/\n",
        "* https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "* https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/\n",
        "* https://imasters.com.br/data/um-mergulho-profundo-nas-redes-neurais-recorrentes\n",
        "\n",
        "\n",
        "# Referências das Imagens\n",
        "\n",
        "* https://www.brainlatam.com/blog/neurolab-6-o-que-sao-os-neuronios-e-como-eles-se-comunicam-1118\n",
        "* https://mc.ai/activation-function-and-its-types/\n",
        "* https://imasters.com.br/data/um-mergulho-profundo-nas-redes-neurais-recorrentes\n",
        "* https://embarc.org/embarc_mli/doc/build/html/MLI_kernels/pooling_max.html\n",
        "\n"
      ]
    }
  ]
}